---
layout:      single
title:       "Neural Networks Hyperparameter Search, the Visualized Way"
excerpt:     "Track and visualize Machine Learning experiments using HiPlot Parallel Coordinates Plot in Python"
description: "We learn how to build a simple model in Keras and track its performance as a function of its hyperparameters.
In end we use HiPlot to make interactive visualization of the hyperparameters"
date:        2021-12-19 09:00:00
classes:     wide
header:
    teaser: "assets/images/hiplot_hyperparams_teaser.png"
    image: "assets/images/hiplot_hyperparams_teaser.png"
    og_image: "assets/images/hiplot_hyperparams_teaser.png"
---

<p>
    In <b>Machine Learning (ML)</b> out-of-the-shelf models are not always available. In many instances, 
    we need to train a model on a specific task. But as in every optimization problem, 
    <a href="https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunchs" target="_blank" rel="noopener">"there ain't no such thing as a free lunch"</a>. 
    Thus, we have to find the model that performs well on our task. 
</p>

<p>
    The ML models, especially the Neural Networks, are characterized by their set of <b>hyperparameters</b>
    that control the learning process. For this reason, the <b>performance</b> of an ML model
    heavily depends on the hyperparameter values. One set of values may result in better performance than another
    set. This search of hyperparameter values is known as hyperparameter optimization.
</p>

<p>
    In this blog we will see how to easily keep track of the model's performance depending on the hyperparameter values
    in a visualized way. First, we will build a simple neural network using <a href="https://keras.io/" target="_blank" rel="noopener nofollow">Keras</a>.
    We will train this network on a sentiment analysis task for many combinations of the hyperparameters.
</p>

<p>
    Finally, we will see how to use the <a href="https://facebookresearch.github.io/hiplot/index.html" target="_blank" rel="noopener nofollow">HiPlot</a>
    library to build an interactive visualization and search for optimal values. Stay tuned!
</p>


<h1>Just Another Keras Model</h1>

<p>
    For demonstration purposes, we build a simple model in <a href="https://keras.io/" target="_blank" rel="noopener nofollow">Keras</a> trained on the 
    <a href="https://keras.io/api/datasets/imdb/" target="_blank" rel="noopener nofollow">IMDB Sentiment Analysis Dataset</a>.
</p>

<h2>Loading the Data</h2>

<p>
    The <a href="https://keras.io/api/datasets/" target="_blank" rel="noopener nofollow">Keras Dataset</a> module provides a few preprocessed and vectorized
    datasets ready to use. The <i>IMDB Sentiment Analysis Dataset</i> contains already processed and tokenized sentences (each word has a unique ID) coupled
    with a label, either 1 indicating a positive sentiment or 0 for negative sentiment. To load it, we use the following Python code:
</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%">1
2
3
4
5
6
7
8
9
10
11</pre></td><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.datasets</span> <span style="color: #008800; font-weight: bold">import</span> imdb
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.preprocessing</span> <span style="color: #008800; font-weight: bold">import</span> sequence

max_features = <span style="color: #0000DD; font-weight: bold">20000</span>  <span style="color: #888888"># vocabulary size</span>
maxlen = <span style="color: #0000DD; font-weight: bold">100</span>  <span style="color: #888888"># max length of every input sequence</span>

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train, y_train = x_train[:<span style="color: #0000DD; font-weight: bold">2500</span>], y_train[:<span style="color: #0000DD; font-weight: bold">2500</span>]
x_test, y_test = x_test[:<span style="color: #0000DD; font-weight: bold">1000</span>], y_test[:<span style="color: #0000DD; font-weight: bold">1000</span>]
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
</pre></td></tr></table></div>
   

<h2>Building the Model</h2>

<p>
    The machine learning model we build is a typical Neural Network architecture used in many text classification tasks.
    It includes the following layers:
</p>

<ul>
    <li><a href="https://keras.io/api/layers/core_layers/embedding/" target="_blank" rel="noopener nofollow">Embedding layer</a> with hyperparameter <b><i>embedding_dim</i></b> indicating the dimensionality of the resulting embeddings;</li>
    <li><a href="https://keras.io/api/layers/regularization_layers/dropout/" target="_blank" rel="noopener nofollow">Dropout layer</a> with hyperparameter <b><i>dropout</i></b> indicating the dropout rate;</li>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution1d/" target="_blank" rel="noopener nofollow">1D Convolution</a> with hyperparameters <b><i>filters</i></b> and <b><i>kernel_size</i></b> defining the number of output channels and the width of the 1D kernel respectively;</li>
    <li><a href="https://keras.io/api/layers/recurrent_layers/lstm/" target="_blank" rel="noopener nofollow">bi-LSTM</a> layer with hyperparameter <b><i>lstm_output_size</i></b> for the dimensionality of the output and</li>
    <li><a href="https://keras.io/api/layers/core_layers/dense/" target="_blank" rel="noopener nofollow">Dense</a> layer with only one output and sigmoid activation.</li>
</ul>

<p>
    The following Python snippet demonstrates what we just described above:
</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></td><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.models</span> <span style="color: #008800; font-weight: bold">import</span> Sequential
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.layers</span> <span style="color: #008800; font-weight: bold">import</span> Activation, Bidirectional, Conv1D, Dense
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.layers</span> <span style="color: #008800; font-weight: bold">import</span> Dropout, Embedding, LSTM, MaxPooling1D


<span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066bb; font-weight: bold">make_model</span>(
    embedding_dim: <span style="color: #003388">int</span>,
    dropout: <span style="color: #003388">float</span>,
    filters: <span style="color: #003388">int</span>,
    kernel_size: <span style="color: #003388">int</span>,
    pool_size: <span style="color: #003388">int</span>,
    lstm_output_size: <span style="color: #003388">int</span>,
    metrics: <span style="color: #003388">list</span>,
    vocab_size: <span style="color: #003388">int</span>,
    maxlen: <span style="color: #003388">int</span>,
):
    model = Sequential(
        [
            Embedding(vocab_size, embedding_dim, input_length=maxlen),
            Dropout(dropout),
            Conv1D(filters, kernel_size, padding=<span style="color: #dd2200; background-color: #fff0f0">&quot;valid&quot;</span>, activation=<span style="color: #dd2200; background-color: #fff0f0">&quot;relu&quot;</span>),
            MaxPooling1D(pool_size=pool_size),
            Bidirectional(LSTM(lstm_output_size), merge_mode=<span style="color: #dd2200; background-color: #fff0f0">&quot;ave&quot;</span>),
            Dense(<span style="color: #0000DD; font-weight: bold">1</span>),
            Activation(<span style="color: #dd2200; background-color: #fff0f0">&quot;sigmoid&quot;</span>),
        ]
    )

    model.compile(optimizer=<span style="color: #dd2200; background-color: #fff0f0">&quot;adam&quot;</span>, loss=<span style="color: #dd2200; background-color: #fff0f0">&quot;binary_crossentropy&quot;</span>, metrics=metrics)
    <span style="color: #008800; font-weight: bold">return</span> model
</pre></td></tr></table></div>
   

<h2>Hyperparameter Search</h2>

<p>
    To measure the impact of the hyperparameters we must define a set of <b>performance metrics</b>. By default, we track the training and validation loss, which in this case is the binary
    <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener nofollow">cross-entropy</a>. On top of this, we will trace the <i>accuracy</i>, <i>precision</i>, and <i>recall</i>.
    In general, it is useful to benchmark the model on multiple metrics. Depending on the use case we might prioritize one over another and at the same time observe the dependency between them.
</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16</pre></td><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #bb0066; font-weight: bold">keras.metrics</span> <span style="color: #008800; font-weight: bold">import</span> BinaryAccuracy, Precision, Recall

METRICS = [
    BinaryAccuracy(name=<span style="color: #dd2200; background-color: #fff0f0">&#39;accuracy&#39;</span>),
    Precision(name=<span style="color: #dd2200; background-color: #fff0f0">&#39;precision&#39;</span>),
    Recall(name=<span style="color: #dd2200; background-color: #fff0f0">&#39;recall&#39;</span>),
]  <span style="color: #888888"># metrics to track</span>

<span style="color: #888888"># hyperparameters to track</span>
embedding_size = [<span style="color: #0000DD; font-weight: bold">32</span>, <span style="color: #0000DD; font-weight: bold">128</span>]
dropout = [<span style="color: #0000DD; font-weight: bold">0.01</span>, <span style="color: #0000DD; font-weight: bold">0.1</span>]
filters = [<span style="color: #0000DD; font-weight: bold">16</span>, <span style="color: #0000DD; font-weight: bold">32</span>, <span style="color: #0000DD; font-weight: bold">64</span>]
kernel_size = [<span style="color: #0000DD; font-weight: bold">3</span>, <span style="color: #0000DD; font-weight: bold">5</span>, <span style="color: #0000DD; font-weight: bold">7</span>]
pool_size = [<span style="color: #0000DD; font-weight: bold">2</span>, <span style="color: #0000DD; font-weight: bold">4</span>]
lstm_output_size = [<span style="color: #0000DD; font-weight: bold">16</span>, <span style="color: #0000DD; font-weight: bold">64</span>]
batch_size = [<span style="color: #0000DD; font-weight: bold">8</span>, <span style="color: #0000DD; font-weight: bold">16</span>, <span style="color: #0000DD; font-weight: bold">32</span>]
</pre></td></tr></table></div>
<br/>
   
<p>
    Once we have defined the hyperparameters to track coupled with the performance metrics, we can start the
    hyperparameter search by plugging-in various combinations of values. In this sense, we create a hypergrid
    from the hyperparameter values. For each point on this hypergrid, we train and evaluate the model. We can
    think of this as one <strong>experiment</strong>, which is usually the case.
</p>

<p>
    As we run the experiments, we log the model performance as a function of the hyperparameter values as one row
    in some external database or file. This is illustrated with the following snippet:
</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32</pre></td><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">import</span> <span style="color: #bb0066; font-weight: bold">itertools</span>

epochs = <span style="color: #0000DD; font-weight: bold">3</span>  <span style="color: #888888"># number of training epochs</span>
test_batch_size = <span style="color: #0000DD; font-weight: bold">32</span>  <span style="color: #888888"># batch size for testing</span>
arrays = [
    embedding_size,
    dropout,
    filters,
    kernel_size,
    pool_size,
    lstm_output_size,
    batch_size,
]  <span style="color: #888888"># all hyper-params</span>

<span style="color: #008800; font-weight: bold">for</span> ed, d, flt, ks, ps, ls, bs <span style="color: #008800">in</span> itertools.product(*arrays):
    model = make_model(
        embedding_dim=ed,
        dropout=d,
        filters=flt,
        kernel_size=ks,
        pool_size=ps,
        lstm_output_size=ls,
        metrics=METRICS,
        vocab_size=max_features,
        maxlen=maxlen,
    )
    h = model.fit(x_train, y_train, batch_size=bs, epochs=epochs, verbose=<span style="color: #0000DD; font-weight: bold">2</span>)
    train_loss = h.history[<span style="color: #dd2200; background-color: #fff0f0">&quot;loss&quot;</span>][-<span style="color: #0000DD; font-weight: bold">1</span>]
    test_metrics = model.evaluate(x=x_test, y=y_test, batch_size=test_batch_size)
    test_loss, test_acc, test_prec, test_rec = test_metrics

    <span style="color: #888888"># write everything to external JSON file</span>
</pre></td></tr></table></div>
<br/>

<p>
    Now that we have generated metadata for our experiments, we have to make it actionable.
</p>

<h1>Visualize the Hyperparameters Impact</h1>

<p>
    Data in raw format is difficult, sometimes impossible to interpret. This especially holds for multivariate
    data!
</p>

<p>
    We can easily resolve this by using the <a href="{{ site.baseurl }}{% link _blog/2020-02-08-interactive-dataviz.html %}" target="_blank" rel="dofollow">parallel coordinates plot</a>.
    With this type of plot, the data dimensions (a.k.a. features) are represented by parallel axes, one per dimension. Thus, each multivariate point is manifested as a poly-line
    connecting the corresponding dimensions. At the same time, this plot encodes the correlation between the data dimensions: line crossings indicate inverse correlation.
    One example of a parallel coordinates plot is shown below:
</p>

<center>
    <picture>
        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/regular/parallel_coordinates_example_regular.webp" media="(min-width: 1281px)" type="image/webp"/>
        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/regular/parallel_coordinates_example_regular.png" media="(min-width: 1281px)" type="image/png"/>

        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/tablet/parallel_coordinates_example_tablet.webp" media="(min-width: 641px) and (max-width: 1280px) and (orientation: landscape)" type="image/webp"/>
        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/tablet/parallel_coordinates_example_tablet.png" media="(min-width: 641px) and (max-width: 1280px) and (orientation: landscape)" type="image/png"/>

        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/tablet/parallel_coordinates_example_tablet.webp" media="(min-width: 641px) and (max-width: 1280px) and (orientation: portrait)" type="image/webp"/>
        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/tablet/parallel_coordinates_example_tablet.png" media="(min-width: 641px) and (max-width: 1280px) and (orientation: portrait)" type="image/png"/>

        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/mobile/parallel_coordinates_example_mobile.webp" media="(max-width: 640px)" type="image/webp"/>
        <source data-srcset="{{ site.url }}{{ site.baseurl }}/assets/images/mobile/parallel_coordinates_example_mobile.png" media="(max-width: 640px)" type="image/png"/>

        <img data-src="{{ site.url }}{{ site.baseurl }}/assets/images/regular/parallel_coordinates_example_regular.png" class="lazyload" alt="Parallel Coordinates Example plot">
    </picture>
    <span class="caption text-muted"><i>Figure 1. Credits: A <a href="https://bl.ocks.org/jasondavies/raw/1341281/" target="_blank" rel=”noopener”>Parallel Coordinates</a> plot
        from <a href="https://bl.ocks.org/" target="_blank" rel=”noopener”>Blocks</a></i>.
    </span>
</center>
<br/>

<p>
    For seamless creation of interactive <i>parallel coordinates plots</i>, we can use <a href="https://facebookresearch.github.io/hiplot/index.html" target="_blank" rel="noopener nofollow">HiPlot</a>,
    an open-source Python library. Given the data that follows a consistent and predefined schema, it automatically generates <i>parallel coordinates plot.</i> The plot can be easily integrated
    into a <a href="https://jupyter.org/" target="_blank" rel="noopener nofollow">Jypyter Notebook</a>, as a standalone HTML file, or directly in a <a href="https://streamlit.io/" target="_blank" rel="noopener nofollow">Streamlit</a> app.
</p>

<p>
    In our case, we have generated metadata for all experiments and we have to make it actionable. Ultimately, we want to see a summary of the hyperparameters' influence
    on the model performance and look at what suits our case.
</p>

<p>
    By just running two lines of code such as:
</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%">1
2</pre></td><td style="border-bottom: none;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">import</span> <span style="color: #bb0066; font-weight: bold">hiplot</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #bb0066; font-weight: bold">hip</span>
hip.Experiment.from_iterable(hiplt_data).display()
</pre></td></tr></table></div>
<br/>

<p>
    we obtain this nice interactive plot as depicted below. Go on, give it a try and see how it works!
</p>

<iframe src="{{ site.url }}{{ site.baseurl }}/assets/html/hiplot.html?hip.color_by=%22test_prec%22&amp;hip.PARALLEL_PLOT.height=350" height="860px" style="width:100%;">
</iframe>
<br/>

<p>
    The benefit of using this interactive plot is that we have a clear overview of all experiments which can be additionally indexed with a unique ID.
</p>

<p>
    In many practical situations, for model reproducibility reasons it is advisable to assign traceable IDs to the experiments, meaning we can always
    roll back and reproduce the same results. For example, if the model and the hyperparameter values are tracked with 
    <i><a href="https://www.git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F" target="_blank" rel="noopener nofollow">git</a></i>, as an experiment ID
    we can use the SHA code of the commit encapsulating the latest repository modifications before running the experiment.
</p>

<p>
    Obviously one disadvantage of this technique is the computational cost. Sometimes it is not affordable to run a plethora of
    experiments just to find the best hyperparameter values. However, over a longer time range, it is possible that the number of
    experiments will become significant. Therefore, in order not to lose any knowledge, it is still better to log the experiments
    and eventually visualize them with <i>HiPlot</i>. 
</p>

<p>
    The source code for the implementation can be found on <a href="https://github.com/IlievskiV/Amusive-Blogging-N-Coding/blob/master/Hyperparameters%20Search/HiPlot_Tutorial.ipynb" target="_blank">GitHub</a>.
    If this is something you like and would like to see similar content you could follow me on <a href="https://www.linkedin.com/in/vilievski/" target="_blank" rel="noopener">LinkedIn</a>
    or <a href="https://twitter.com/VladOsaurus" target="_blank" rel="noopener">Twitter</a>. Additionally, you can subscribe to the mailing list below.
</p>

<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<link href="/assets/css/mailchimp.css">
<div id="mc_embed_signup">
<form action="https://digital.us19.list-manage.com/subscribe/post?u=cb9dbe40387c27177a25de80f&amp;id=08bda6f8e0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<label for="mce-EMAIL">Join the iSquared mailing list</label>
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cb9dbe40387c27177a25de80f_08bda6f8e0" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<h1>Summary</h1>

<p>
    In this blog we learned how to make our machine learning experiments more useful with a visualization
    technique called <i>parallel coordinates plot.</i>
</p>

<p>
    We tracked and logged the performance of one simple Keras model depending on the hyperparameter values. Later we
    made this metadata actionable using <a href="https://facebookresearch.github.io/hiplot/index.html" target="_blank" rel="noopener nofollow">HiPlot</a>, 
    an open-source Python library for creating interactive <i>parallel coordinates plots.</i>
</p>